{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirPath = r\"\\data\"\n",
    "reviewPath = r\"reviews_Tools_and_Home_Improvement_5.json.gz\"\n",
    "#reviewPath = r\"reviews_Musical_Instruments_5.json.gz\"\n",
    "reviewPath = r\"cleanData_reviews_Musical_Instruments_5.json.gz\"\n",
    "path = dirPath + \"\\\\\" + reviewPath\n",
    "path = \"preprocessed_reviews_Musical_Instruments_5.pkl.gz\"\n",
    "path = \"preprocessed_reviews_Tools_and_Home_Improvement_5.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123707, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(path)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10105"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negativeReviews = data.loc[data['trueRating'] == 0, :]\n",
    "negativeReviews.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.iloc[0:size*5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masterIndex</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>trueRating</th>\n",
       "      <th>CleanedJoin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>123477</td>\n",
       "      <td>134239</td>\n",
       "      <td>B00JPBDL9W</td>\n",
       "      <td>I have converted three-quarters of my home to ...</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i have convert three quarter of my home to lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123556</td>\n",
       "      <td>134322</td>\n",
       "      <td>B00JXSASK2</td>\n",
       "      <td>Amazing that they designed this thing without ...</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>amaze that they design this thing without real...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        masterIndex        asin  \\\n",
       "123477       134239  B00JPBDL9W   \n",
       "123556       134322  B00JXSASK2   \n",
       "\n",
       "                                               reviewText helpful  overall  \\\n",
       "123477  I have converted three-quarters of my home to ...  [0, 0]      2.0   \n",
       "123556  Amazing that they designed this thing without ...  [2, 2]      1.0   \n",
       "\n",
       "        trueRating                                        CleanedJoin  \n",
       "123477         0.0  i have convert three quarter of my home to lea...  \n",
       "123556         0.0  amaze that they design this thing without real...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negativeReviews.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masterIndex</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>trueRating</th>\n",
       "      <th>CleanedJoin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>123556</td>\n",
       "      <td>134322</td>\n",
       "      <td>B00JXSASK2</td>\n",
       "      <td>Amazing that they designed this thing without ...</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>amaze that they design this thing without real...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        masterIndex        asin  \\\n",
       "123556       134322  B00JXSASK2   \n",
       "\n",
       "                                               reviewText helpful  overall  \\\n",
       "123556  Amazing that they designed this thing without ...  [2, 2]      1.0   \n",
       "\n",
       "        trueRating                                        CleanedJoin  \n",
       "123556         0.0  amaze that they design this thing without real...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pad more negative reviews for better training purposes\n",
    "testSample = pd.concat([data, negativeReviews])\n",
    "#testSample = data\n",
    "#testSample.reset_index(drop=True, inplace=True)\n",
    "testSample.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133812"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "largeText = testSample.CleanedJoin.to_list()\n",
    "#largeText = reviewTextProcessing.reviewText.to_list() # Not too good accuracy around 116 missed with %86.94\n",
    "\n",
    "len(largeText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(largeText)\n",
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = largeText\n",
    "y = testSample['trueRating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=310)\n",
    "\n",
    "x_train_text = X_train\n",
    "x_test_text = X_test\n",
    "\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train_text)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test_text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#  not very good way to arrange the samples \n",
    "size = int(len(largeText)*.75)\n",
    "x_train_text = largeText[:size]\n",
    "x_test_text = largeText[size:]\n",
    "\n",
    "y_train =  reviewTextProcessing['rating'][:size]\n",
    "y_test = reviewTextProcessing['rating'][size:]\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train_text)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100359, 33453, 100359, 33453)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train), len(y_test), len(x_train_text), len(x_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_tokens[1], x_train_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train_text[1]\n",
    "np.array(x_train_tokens[1])\n",
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111.61465339431442, 4959)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens), np.max(num_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133.85901020345165"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens) \n",
    "max_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9588078797118346"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(num_tokens < max_tokens) / len(num_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100359, 379), (33453, 379))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad = 'pre'\n",
    "#pad = 'post'\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens, padding=pad, truncating=pad)\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens, padding=pad, truncating=pad)\n",
    "x_train_pad.shape, x_test_pad.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0   28\n",
      "  248  706   10   11    1   79    8 1209    1 1466   12   49  846  861\n",
      "   53   40    6   52  177    1   47 1209    4    2   59    6   45  166\n",
      "   23    3   44    8   12    8   89  531   16    1   53  373  324    5\n",
      "    2   32  302    6   19    1 1391   22   18   75   26    2  602    6\n",
      "   18]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 4):\n",
    "    print(x_train_pad[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    \n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'very handy and easy to use, these arrive on time and in good condition, nice to have in shop when you need it you need it'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'very handy and easy to use these arrive on time and in good condition nice to have in shop when you need it you need it'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100359, 33453, 133812)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = len(x_train_pad)\n",
    "index\n",
    "len(y_train), len(y_test), len(y_train) + len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'modelCNNImplementation' from '/home/toan/Study/NLP/modelCNNImplementation.py'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#abstract CNN\n",
    "import modelCNNImplementation as modelCNN\n",
    "import importlib\n",
    "importlib.reload(modelCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'modelRNNImplementation' from '/home/toan/Study/NLP/modelRNNImplementation.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import modelRNNImplementation as modelRNN\n",
    "import importlib\n",
    "importlib.reload(modelRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep seed consistently \n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import random\n",
    "random.set_seed(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toan/.pyenv/versions/3.7.3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95341 samples, validate on 5018 samples\n",
      "Epoch 1/3\n",
      "95341/95341 [==============================] - 13s 136us/step - loss: 0.2256 - accuracy: 0.9105 - val_loss: 0.1703 - val_accuracy: 0.9299\n",
      "Epoch 2/3\n",
      "95341/95341 [==============================] - 13s 141us/step - loss: 0.1252 - accuracy: 0.9526 - val_loss: 0.1524 - val_accuracy: 0.9426\n",
      "Epoch 3/3\n",
      "95341/95341 [==============================] - 13s 140us/step - loss: 0.0749 - accuracy: 0.9725 - val_loss: 0.1770 - val_accuracy: 0.9394\n",
      "batch size 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toan/.pyenv/versions/3.7.3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95341 samples, validate on 5018 samples\n",
      "Epoch 1/3\n",
      "95341/95341 [==============================] - 14s 143us/step - loss: 0.2235 - accuracy: 0.9110 - val_loss: 0.1713 - val_accuracy: 0.9314\n",
      "Epoch 2/3\n",
      "95341/95341 [==============================] - 14s 144us/step - loss: 0.1230 - accuracy: 0.9532 - val_loss: 0.1404 - val_accuracy: 0.9492\n",
      "Epoch 3/3\n",
      "95341/95341 [==============================] - 13s 140us/step - loss: 0.0708 - accuracy: 0.9743 - val_loss: 0.1647 - val_accuracy: 0.9352\n",
      "batch size 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toan/.pyenv/versions/3.7.3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95341 samples, validate on 5018 samples\n",
      "Epoch 1/3\n",
      "95341/95341 [==============================] - 14s 143us/step - loss: 0.2242 - accuracy: 0.9111 - val_loss: 0.1637 - val_accuracy: 0.9312\n",
      "Epoch 2/3\n",
      "95341/95341 [==============================] - 13s 141us/step - loss: 0.1229 - accuracy: 0.9532 - val_loss: 0.1394 - val_accuracy: 0.9462\n",
      "Epoch 3/3\n",
      "95341/95341 [==============================] - 14s 142us/step - loss: 0.0720 - accuracy: 0.9743 - val_loss: 0.1455 - val_accuracy: 0.9496\n",
      "batch size 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toan/.pyenv/versions/3.7.3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95341 samples, validate on 5018 samples\n",
      "Epoch 1/3\n",
      "95341/95341 [==============================] - 14s 143us/step - loss: 0.2291 - accuracy: 0.9091 - val_loss: 0.1644 - val_accuracy: 0.9348\n",
      "Epoch 2/3\n",
      "95341/95341 [==============================] - 13s 138us/step - loss: 0.1285 - accuracy: 0.9508 - val_loss: 0.1445 - val_accuracy: 0.9422\n",
      "Epoch 3/3\n",
      "95341/95341 [==============================] - 13s 139us/step - loss: 0.0774 - accuracy: 0.9723 - val_loss: 0.1422 - val_accuracy: 0.9466\n",
      "batch size 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toan/.pyenv/versions/3.7.3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95341 samples, validate on 5018 samples\n",
      "Epoch 1/3\n",
      "95341/95341 [==============================] - 14s 142us/step - loss: 0.2263 - accuracy: 0.9111 - val_loss: 0.1649 - val_accuracy: 0.9312\n",
      "Epoch 2/3\n",
      "95341/95341 [==============================] - 13s 139us/step - loss: 0.1193 - accuracy: 0.9552 - val_loss: 0.1428 - val_accuracy: 0.9442\n",
      "Epoch 3/3\n",
      "95341/95341 [==============================] - 14s 142us/step - loss: 0.0665 - accuracy: 0.9770 - val_loss: 0.1392 - val_accuracy: 0.9536\n",
      "batch size 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toan/.pyenv/versions/3.7.3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95341 samples, validate on 5018 samples\n",
      "Epoch 1/3\n",
      "95341/95341 [==============================] - 14s 142us/step - loss: 0.2325 - accuracy: 0.9069 - val_loss: 0.1755 - val_accuracy: 0.9297\n",
      "Epoch 2/3\n",
      "95341/95341 [==============================] - 13s 141us/step - loss: 0.1285 - accuracy: 0.9506 - val_loss: 0.1568 - val_accuracy: 0.9388\n",
      "Epoch 3/3\n",
      "95341/95341 [==============================] - 13s 141us/step - loss: 0.0741 - accuracy: 0.9729 - val_loss: 0.1460 - val_accuracy: 0.9530\n",
      "batch size 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toan/.pyenv/versions/3.7.3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95341 samples, validate on 5018 samples\n",
      "Epoch 1/3\n",
      "95341/95341 [==============================] - 14s 143us/step - loss: 0.2240 - accuracy: 0.9117 - val_loss: 0.1807 - val_accuracy: 0.9279\n",
      "Epoch 2/3\n",
      "95341/95341 [==============================] - 13s 140us/step - loss: 0.1226 - accuracy: 0.9530 - val_loss: 0.1394 - val_accuracy: 0.9484\n",
      "Epoch 3/3\n",
      "95341/95341 [==============================] - 13s 141us/step - loss: 0.0690 - accuracy: 0.9752 - val_loss: 0.1341 - val_accuracy: 0.9530\n",
      "batch size 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toan/.pyenv/versions/3.7.3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95341 samples, validate on 5018 samples\n",
      "Epoch 1/3\n",
      "95341/95341 [==============================] - 14s 142us/step - loss: 0.2294 - accuracy: 0.9082 - val_loss: 0.1579 - val_accuracy: 0.9374\n",
      "Epoch 2/3\n",
      "95341/95341 [==============================] - 13s 140us/step - loss: 0.1259 - accuracy: 0.9526 - val_loss: 0.1499 - val_accuracy: 0.9406\n",
      "Epoch 3/3\n",
      "95341/95341 [==============================] - 13s 139us/step - loss: 0.0721 - accuracy: 0.9733 - val_loss: 0.1273 - val_accuracy: 0.9560\n",
      "batch size 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toan/.pyenv/versions/3.7.3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95341 samples, validate on 5018 samples\n",
      "Epoch 1/3\n",
      "95341/95341 [==============================] - 13s 138us/step - loss: 0.2305 - accuracy: 0.9071 - val_loss: 0.1753 - val_accuracy: 0.9285\n",
      "Epoch 2/3\n",
      "95341/95341 [==============================] - 13s 139us/step - loss: 0.1279 - accuracy: 0.9507 - val_loss: 0.1572 - val_accuracy: 0.9400\n",
      "Epoch 3/3\n",
      "95341/95341 [==============================] - 13s 140us/step - loss: 0.0781 - accuracy: 0.9712 - val_loss: 0.1380 - val_accuracy: 0.9542\n",
      "batch size 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toan/.pyenv/versions/3.7.3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95341 samples, validate on 5018 samples\n",
      "Epoch 1/3\n",
      "95341/95341 [==============================] - 13s 140us/step - loss: 0.2260 - accuracy: 0.9100 - val_loss: 0.1649 - val_accuracy: 0.9352\n",
      "Epoch 2/3\n",
      "95341/95341 [==============================] - 13s 140us/step - loss: 0.1226 - accuracy: 0.9535 - val_loss: 0.1447 - val_accuracy: 0.9432\n",
      "Epoch 3/3\n",
      "95341/95341 [==============================] - 14s 143us/step - loss: 0.0724 - accuracy: 0.9740 - val_loss: 0.1466 - val_accuracy: 0.9460\n"
     ]
    }
   ],
   "source": [
    "# Refactor here to run multiple simulations\n",
    "\n",
    "cls_true = np.array(y_test)\n",
    "\n",
    "numOfSimulations = 10\n",
    "\n",
    "for eachRun in range (0, numOfSimulations):\n",
    "    model_cnn = modelCNN.compileCNNModel(max_tokens=max_tokens)\n",
    "    model_rnn = modelRNN.compileRNNModel(max_tokens=max_tokens, embedding_size=200)\n",
    "    #custom_batch_size=32+eachRun*6\n",
    "    #custom_batch_size=50+eachRun*6\n",
    "    custom_batch_size=56\n",
    "    print(f\"batch size {custom_batch_size}\")\n",
    "    #train models\n",
    "    model_cnn.fit(x_train_pad, y_train, validation_split=0.05, epochs=3, batch_size=custom_batch_size)\n",
    "    #model_rnn.fit(x_train_pad, y_train, validation_split=0.05, epochs=3, batch_size=128)\n",
    "    #predict results\n",
    "    cnn_y_pred = model_cnn.predict(x=x_test_pad)\n",
    "    cnn_y_pred = cnn_y_pred.T[0]\n",
    "    #rnn_y_pred = model_rnn.predict(x=x_test_pad)\n",
    "    #rnn_y_pred = rnn_y_pred.T[0]\n",
    "    cnn_cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in cnn_y_pred])\n",
    "    #rnn_cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in rnn_y_pred])\n",
    "    #result matrix including duplicated results\n",
    "    y_test.index\n",
    "    y_test.values\n",
    "    result = pd.DataFrame()\n",
    "    result['Index'] = y_test.index\n",
    "    result['TrueRating'] = y_test.values\n",
    "    result['CNNRating'] = cnn_cls_pred\n",
    "    #result['RNNRating'] = rnn_cls_pred\n",
    "    # need to drop the duplicates due to negative padding\n",
    "    result.drop_duplicates(subset=\"Index\", inplace=True)\n",
    "    #merge results  back\n",
    "    df = testSample.loc[result.Index.to_list(), :]\n",
    "    df = df.reset_index()\n",
    "    df= df.drop_duplicates(subset=\"index\")\n",
    "    df.rename(columns={\"index\": \"Index\"}, inplace=True)\n",
    "    modelResult = pd.merge(df, result, on=\"Index\", how=\"inner\")\n",
    "    y_true = modelResult.TrueRating.to_list()\n",
    "    cnn_pred = modelResult.CNNRating.to_list()\n",
    "    #rnn_pred = modelResult.RNNRating.to_list() \n",
    "    report = dict()\n",
    "    report['CNN'] = confusion_matrix(y_true, cnn_pred).ravel()\n",
    "    #report['RNN'] = confusion_matrix(y_true, rnn_pred).ravel()\n",
    "    y_true = np.array(y_true)\n",
    "    reportCard = pd.DataFrame(report).T\n",
    "    reportCard.columns = ['tn', 'fp', 'fn', 'tp']\n",
    "    reportCard.loc['ActualRating'] = [len(np.where(y_true==0)[0]), 0, 0, len(np.where(y_true==1)[0])]\n",
    "    reportCard.loc['custom_batch_size'] = custom_batch_size\n",
    "    reportCard.to_csv(f\"run{eachRun}_PaddingNegativeReviews_CNN.csv\")\n",
    "    del model_cnn\n",
    "    del model_rnn  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = modelCNN.compileCNNModel(max_tokens=max_tokens)\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = modelRNN.compileRNNModel(max_tokens=max_tokens, embedding_size=200)\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_cnn.fit(x_train_pad, y_train, validation_split=0.05, epochs=3, batch_size=64)\n",
    "#model.fit(x_train_pad, y_train,validation_split=0.05, epochs=3, batch_size=74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess = tf.session(config=tf.ConfigProto(log_device_placement=True))\n",
    "tf.test.gpu_device_name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_rnn.fit(x_train_pad, y_train, validation_split=0.05, epochs=3, batch_size=128)\n",
    "#model.fit(x_train_pad, y_train,validation_split=0.05, epochs=3, batch_size=74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "cnn_result = model_cnn.evaluate(x_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "rnn_result = model_rnn.evaluate(x_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: {0:.2%}\".format(rnn_result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: {0:.2%}\".format(cnn_result[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cnn_y_pred = model_cnn.predict(x=x_test_pad)\n",
    "cnn_y_pred = cnn_y_pred.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rnn_y_pred = model_rnn.predict(x=x_test_pad)\n",
    "rnn_y_pred = rnn_y_pred.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cnn_y_pred).describe(), pd.DataFrame(rnn_y_pred).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_y_pred.shape, rnn_y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in cnn_y_pred])\n",
    "rnn_cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in rnn_y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cnn_cls_pred), len(rnn_cls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_accuracy = (cnn_cls_pred==y_test).mean()\n",
    "rnn_accuracy = (rnn_cls_pred==y_test).mean()\n",
    "cnn_accuracy, rnn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for padding\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, cnn_cls_pred).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, rnn_cls_pred).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(y_test==0)[0]),len(np.where(y_test==1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(y_train==0)[0]),len(np.where(y_train==1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_true = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.index\n",
    "y_test.values\n",
    "result = pd.DataFrame()\n",
    "result['Index'] = y_test.index\n",
    "result['TrueRating'] = y_test.values\n",
    "result['CNNRating'] = cnn_cls_pred\n",
    "result['RNNRating'] = rnn_cls_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to drop the duplicates due to negative padding\n",
    "result.drop_duplicates(subset=\"Index\", inplace=True)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = testSample.loc[result.Index.to_list(), :]\n",
    "df = df.reset_index()\n",
    "#A.drop_duplicates()\n",
    "df= df.drop_duplicates(subset=\"index\")\n",
    "df.rename(columns={\"index\": \"Index\"}, inplace=True)\n",
    "df.shape, df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelResult = pd.merge(df, result, on=\"Index\", how=\"inner\")\n",
    "modelResult.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TextBlobImplementation as TextBlob\n",
    "import importlib\n",
    "importlib.reload(TextBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polaritySample = modelResult.iloc[3, :]['CleanedJoin']\n",
    "\n",
    "TextBlob.getPolarityFromText(polaritySample.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TextPreProcessing as TextProcessing\n",
    "import importlib\n",
    "importlib.reload(TextProcessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "modelResult['simplifiedText'] = modelResult.reviewText.apply(TextProcessing.simplifiedTextParagraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#not very accurate using stemming\n",
    "#modelResult['textBlobRating'] = modelResult.CleanedJoin.str.split(',')\n",
    "modelResult['textBlobRating'] = modelResult.simplifiedText.apply(TextBlob.getPolarityFromText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelResult.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true = modelResult.TrueRating.to_list()\n",
    "cnn_pred = modelResult.CNNRating.to_list()\n",
    "textBlob_pred = modelResult.textBlobRating.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = modelResult.TrueRating.to_list()\n",
    "rnn_pred = modelResult.RNNRating.to_list() \n",
    "textBlob_pred = modelResult.textBlobRating.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report = {'CNN': [], 'RNN': None, 'TextBlob':None}\n",
    "report = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tn, fp, fn, tp = confusion_matrix(y_true, cnn_pred).ravel()\n",
    "report['CNN'] = confusion_matrix(y_true, cnn_pred).ravel()\n",
    "#tn, fp, fn, tp\n",
    "report['CNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tn, fp, fn, tp = confusion_matrix(y_true, rnn_pred).ravel()\n",
    "report['RNN'] = confusion_matrix(y_true, rnn_pred).ravel()\n",
    "#tn, fp, fn, tp\n",
    "report['RNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tn, fp, fn, tp = confusion_matrix(y_true, textBlob_pred).ravel()\n",
    "report['TextBlob'] = confusion_matrix(y_true, textBlob_pred).ravel()\n",
    "#tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 50\n",
    "\n",
    "#productsReviewData.cleanedText.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportCard = pd.DataFrame(report).T\n",
    "reportCard.columns = ['tn', 'fp', 'fn', 'tp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportCard.loc['ActualRating'] = [len(np.where(y_true==0)[0]), 0, 0, len(np.where(y_true==1)[0])]\n",
    "reportCard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportCard.to_csv(\"2nd_PaddingNegativeReviews_RNN_CNN_TextBlob.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 133K review comparison\n",
    "\n",
    "## padding extra negative reviews\n",
    "\n",
    "##### we pick up two unique negative extras with the sampling when we padded\n",
    "\n",
    "tn, fp, fn, tp\n",
    "\n",
    "(3913, 554, 742, 27627)  CNN for negative reviews padding \n",
    "(3445, 1022, 923, 27446) RNN with negative reviews padding\n",
    "(1142, 3325, 1782, 26587) textBlob with negative reviews padding \n",
    "\n",
    "Note: RNN 3*8 word vector with  batch size = 64\n",
    "\n",
    "## no padding extra negative reviews\n",
    "\n",
    "tn, fp, fn, tp\n",
    "\n",
    "(1667, 916, 813, 27531)  CNN without negative reviews padding\n",
    "(1034, 1549, 270, 28074) RNN without negative reviews padding\n",
    "(671, 1912, 1767, 26577) textBlob without negative reviews padding\n",
    "\n",
    "\n",
    "## Multinomial Naive Bayes no padding extra negative reviews\n",
    "\n",
    "tn, fp, fn, tp\n",
    "\n",
    "(509, 2074, 4175, 24169) Multinomial Naive Bayes # n-gram (1,2), num_words = 20000\n",
    "(26, 83, 393, 1871) Multinomial Naive Bayes # n-gram (1,1), num_words = 3000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 10K review comparison\n",
    "\n",
    "## padding extra negative reviews\n",
    "\n",
    "##### we pick up two unique negative extras with the sampling when we padded\n",
    "\n",
    "tn, fp, fn, tp\n",
    "\n",
    "(174, 27, 44, 2214) CNN for negative reviews padding \n",
    "(113, 88, 34, 2224)  RNN with negative review padding\n",
    "(50, 151, 112, 2146) textBlob with negative padding \n",
    "\n",
    "## no padding extra negative reviews\n",
    "\n",
    "tn, fp, fn, tp\n",
    "\n",
    "(31, 78, 37, 2227)  CNN without negative reviews padding\n",
    "(3, 106, 2, 2262)   RNN without negative reviews padding\n",
    "(27, 82, 106, 2158) textBlob without negative reviews padding\n",
    "\n",
    "\n",
    "## Multinomial Naive Bayes no padding extra negative reviews\n",
    "\n",
    "tn, fp, fn, tp\n",
    "\n",
    "(12, 97, 232, 2032) Multinomial Naive Bayes # n-gram (1,2), num_words = 10000\n",
    "(26, 83, 393, 1871) Multinomial Naive Bayes # n-gram (1,1), num_words = 3000\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
